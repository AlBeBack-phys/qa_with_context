{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74888a5-1939-4583-94c8-735a2e08c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "splits = {'train': 'sberquad/train-00000-of-00001.parquet', 'validation': 'sberquad/validation-00000-of-00001.parquet', 'test': 'sberquad/test-00000-of-00001.parquet'}\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/kuznetsoffandrey/sberquad/\" + splits[\"train\"])\n",
    "df['answer'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "df['answer_start'] = df['answers'].apply(lambda x: x['answer_start'][0])\n",
    "df['answer_end'] = df['answer'].apply(lambda x: len(x)) + df['answer_start']\n",
    "df = df[df['answer_start'] != -1]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[['context', 'question', 'answer_start', 'answer_end']]\n",
    "df['has_answer'] = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573348dd-ecda-4e6c-80d2-a3b0e0d1b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем примеры когда ответ не найден\n",
    "def generate_negative_examples(df):\n",
    "    shuffled_questions = df['question'].sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "    shuffled_question_df = pd.DataFrame({\n",
    "        'context': df['context'],\n",
    "        'question': shuffled_questions\n",
    "    })[:len(df) // 3]\n",
    "\n",
    "    # Cгенерируем примеры без контекста\n",
    "\n",
    "    without_context = df['question'].sample(frac=1).reset_index(drop=True)\n",
    "    without_context_df = pd.DataFrame({\n",
    "        'context': [''] * len(without_context),\n",
    "        'question': without_context\n",
    "    })[:len(df) // 50]\n",
    "\n",
    "    negative_examples = pd.concat([shuffled_question_df, without_context_df], ignore_index=True)\n",
    "\n",
    "    negative_examples['answer_start'] = -1\n",
    "    negative_examples['answer_end'] = -1\n",
    "    negative_examples['has_answer'] = 0\n",
    "\n",
    "    df = pd.concat([df, negative_examples], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    del shuffled_questions, shuffled_question_df, without_context, without_context_df, negative_examples\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_negative_examples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cf93c-b54f-4ef6-a938-3390be0f7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self, transformer_model_name=\"DeepPavlov/rubert-base-cased\"):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
    "\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        self.start_vector = nn.Linear(hidden_size, 1)\n",
    "        self.end_vector = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_output.last_hidden_state\n",
    "\n",
    "        # Воспользуемся токеном начала последовательности для классификации\n",
    "        has_answer = torch.sigmoid(self.classifier(hidden_states[:, 0, :])).squeeze(-1)\n",
    "\n",
    "        start_logits = self.start_vector(hidden_states).squeeze(-1)\n",
    "\n",
    "        start_pred = torch.argmax(start_logits, dim=-1)\n",
    "\n",
    "        # Маскируем на всякий случай токены которые находятся до start_pred\n",
    "        mask = torch.arange(hidden_states.size(1), device=device)[None, :] >= start_pred[:, None]\n",
    "        end_hidden_states = hidden_states * mask[:, :, None]\n",
    "\n",
    "        end_logits = self.end_vector(end_hidden_states).squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits, has_answer\n",
    "\n",
    "model = QAModel().to(device)\n",
    "model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663144f2-1e9a-4a93-b9d0-6a8f8b6b0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "X_train = tokenizer(df['question'].tolist(), \n",
    "                    df['context'].tolist(), \n",
    "                    return_tensors=\"pt\", \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                   )\n",
    "#Определим токены начала и конца ответов\n",
    "y_train = []\n",
    "for i, offset in enumerate(X_train['offset_mapping']):\n",
    "    if df['has_answer'][i] == 0:\n",
    "        y_train.append([-1, -1])\n",
    "        continue\n",
    "\n",
    "    borders = []\n",
    "    count_0 = 0\n",
    "    for j, (start, end) in enumerate(offset):\n",
    "        if (start, end) == (0, 0):\n",
    "            count_0 += 1\n",
    "            if count_0 > 2:\n",
    "                break\n",
    "        if start <= df['answer_start'][i] <= end:\n",
    "            if len(borders) > 0:\n",
    "                borders.pop()\n",
    "            if count_0 == 2: # мы находимся в контексте\n",
    "                borders.append(j)\n",
    "\n",
    "        if start <= df['answer_end'][i] <= end:\n",
    "            if count_0 == 2:\n",
    "                borders.append(j) # мы находимся в контексте\n",
    "                break\n",
    "    \n",
    "    while len(borders) < 2:\n",
    "        borders.append(j - 1)\n",
    "\n",
    "    y_train.append(borders)\n",
    "\n",
    "y_train = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4763d-c9c0-4c2a-884b-359e14071e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, has_answer_labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.has_answer_labels = has_answer_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = [torch.tensor(self.labels[idx][0]), \n",
    "                          torch.tensor(self.labels[idx][1])]\n",
    "        item['has_answer'] = torch.tensor(self.has_answer_labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = CustomDataset(dict(X_train), y_train, df['has_answer'].tolist())\n",
    "loader = DataLoader(dataset, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c72f2-cc99-4eec-8485-24b522e31de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'model_checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "num_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "total_steps  = len(loader) * num_epochs\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion_has_answer = torch.nn.BCELoss()\n",
    "sheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                          num_warmup_steps=total_steps*0.001,\n",
    "                                         num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(loader, unit='batch') as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in tepoch:\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_start = torch.tensor(batch['labels'][0]).to(device)\n",
    "            labels_end = torch.tensor(batch['labels'][1]).to(device)\n",
    "            has_answer = batch['has_answer'].to(device)\n",
    "\n",
    "            start_logits, end_logits, has_answer_prob = model(input_ids=batch['input_ids'], \n",
    "                                                            attention_mask=batch['attention_mask'])\n",
    "\n",
    "            loss_classification = criterion_has_answer(has_answer_prob, has_answer.float())\n",
    "\n",
    "            # Считаем loss классификации начала и конца только по тем объектам где есть ответ\n",
    "            if has_answer.sum() > 0:\n",
    "                mask = has_answer.bool()\n",
    "                start_loss = criterion(start_logits[mask], labels_start[mask])\n",
    "                end_loss = criterion(end_logits[mask], labels_end[mask])\n",
    "                loss = loss_classification + start_loss + end_loss\n",
    "            else:\n",
    "                loss = loss_classification\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "      \n",
    "    # Сохранение модели и оптимизатора после каждой эпохи\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': sheduler.state_dict(),\n",
    "            'loss': epoch_loss / len(loader)\n",
    "      }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} finished with loss: {epoch_loss / len(loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
